

from typing import List
from openai import OpenAI
from .config import settings

# RAG ã®åŸºæœ¬å§¿å‹¢ã‚’ã‚¬ã‚¤ãƒ‰ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
_SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯RAGã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæ–‡è„ˆã ã‘ã‚’æ ¹æ‹ ã«ã€ç°¡æ½”ã§æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚"
    "ã‚ã‹ã‚‰ãªã„å ´åˆã¯æ¨æ¸¬ã›ãšã€æ‰‹å…ƒã®æ–‡æ›¸ã‹ã‚‰ã¯æ–­å®šã§ãã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"
)

# OpenAI äº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆbase_url ã‚’ LM Studio ã«å‘ã‘ã‚‹ï¼‰
_client = OpenAI(base_url=settings.llm_base_url, api_key=settings.llm_api_key)

def rag_answer(query: str, contexts: List[str]) -> str:
    """æ¤œç´¢ã§å¾—ãŸä¸Šä½ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡è„ˆã¨ã—ã¦æ¸¡ã—ã€Phi-3-mini ã§å›ç­”ã‚’ç”Ÿæˆã€‚"""
    context_block = "\n\n".join([f"[CONTEXT {i+1}]\n" + c for i, c in enumerate(contexts)])
    user_prompt = (
        f"è³ªå•:\n{query}\n\n"
        f"å‚è€ƒæ–‡è„ˆ:\n{context_block}\n\n"
        "æ—¥æœ¬èªã§ã€è¦ç‚¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚æ ¹æ‹ ã®ç®‡æ‰€ãŒã‚ã‚Œã°çŸ­ãè§¦ã‚Œã¦ãã ã•ã„ã€‚"
    )

    # ğŸ”¹ ã“ã“ã§ LM Studio ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    resp = _client.chat.completions.create(
        model=settings.llm_model,
        messages=[
            {"role": "system", "content": _SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
        max_tokens=512,
    )

    raw_answer = resp.choices[0].message.content

    # å¿µã®ãŸã‚å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰â†’ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
       raw_answer = raw_answer.encode("latin1").decode("utf-8")
    except Exception as e:
       print("Re-decode failed:", e)

    #  ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    print("=== RAW ANSWER (repr) ===")
    print(repr(raw_answer))

    #  å¿µã®ãŸã‚UTF-8å¤‰æ›
    if isinstance(raw_answer, bytes):
        try:
            raw_answer = raw_answer.decode("utf-8", errors="ignore")
        except Exception as e:
            print("Decode error:", e)

    return str(raw_answer).strip()
